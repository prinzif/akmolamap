"""
/backend/ndvi.py - NDVI –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —á–µ—Ä–µ–∑ Sentinel Hub API.

–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å:
- Statistical API –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
- ORBIT mosaicking –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
- Calculations API –¥–ª—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º –∏ –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª–µ–π
- –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å rasterio
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
"""

import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
from pathlib import Path
import hashlib
import json
import time

import numpy as np

from backend.constants import (
    STATUS_SUCCESS,
    STATUS_ERROR,
    STATUS_NO_DATA,
    NDVI_STATUS_OPTIMAL,
    NDVI_STATUS_HIGH,
    NDVI_STATUS_LOW,
    NDVI_STATUS_CRITICAL_LOW,
    NDVI_STATUS_WATER,
    NDVI_STATUS_BARE_SOIL,
    NDVI_STATUS_DEFAULT,
    NDVI_THRESHOLD_OPTIMAL,
    NDVI_THRESHOLD_HIGH,
    NDVI_THRESHOLD_LOW,
    NDVI_THRESHOLD_CRITICAL
)
from scipy import stats as scipy_stats
import requests

try:
    import rasterio
    from rasterio.windows import Window
    from rasterio.warp import transform_geom
    from rasterio.crs import CRS
except ImportError:
    rasterio = None

from backend.sentinel import search_products
from backend.ndvi_sentinelhub import (
    fetch_ndvi_geotiff,
    get_cdse_token,
    SentinelHubError,
    NoDataAvailableError,
    MosaickingOrder
)

logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç –Ω–∞—Å—Ç—Ä–æ–µ–∫
from backend.settings import settings

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫—ç—à–∞
CACHE_DIR = Path(__file__).resolve().parents[1] / "cache" / "ndvi"
STATS_CACHE_DIR = CACHE_DIR / "stats"
STATS_CACHE_DIR.mkdir(parents=True, exist_ok=True)

# Sentinel Hub API endpoints (–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫)
SH_STATISTICS_URL = settings.SH_STATISTICS_URL
SH_PROCESS_URL = settings.SH_PROCESS_URL


# --------------------------- –£—Ç–∏–ª–∏—Ç—ã --------------------------------- #

def _require_rasterio() -> None:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è rasterio."""
    if rasterio is None:
        raise RuntimeError(
            "rasterio –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–∞–∫–µ—Ç: pip install rasterio"
        )


def _open_ndvi_array(
    tif_path: Path,
    window: Optional[Window] = None
) -> Tuple[np.ndarray, Dict[str, Any]]:
    """
    –ß–∏—Ç–∞–µ—Ç GeoTIFF (1 –∫–∞–Ω–∞–ª, FLOAT32), –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Å—Å–∏–≤ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ.
    
    Args:
        tif_path: –ü—É—Ç—å –∫ GeoTIFF —Ñ–∞–π–ª—É
        window: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è
        
    Returns:
        Tuple[np.ndarray, Dict]: –ú–∞—Å—Å–∏–≤ –¥–∞–Ω–Ω—ã—Ö –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    """
    _require_rasterio()
    
    with rasterio.open(tif_path) as src:
        # –ß–∏—Ç–∞–µ–º —Å masked=True –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ nodata
        arr = src.read(1, window=window, masked=True)
        meta = src.meta.copy()

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy array —Å NaN –≤–º–µ—Å—Ç–æ –º–∞—Å–∫–∏
    data = np.array(arr.filled(np.nan), dtype=np.float32)

    # –Ø–≤–Ω–æ —É–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π masked array –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏
    del arr

    return data, meta


def _sample_point_ndvi(
    tif_path: Path,
    lon: float,
    lat: float
) -> Optional[float]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ NDVI –≤ —Ç–æ—á–∫–µ (lon, lat) –∏–∑ GeoTIFF.
    
    Args:
        tif_path: –ü—É—Ç—å –∫ GeoTIFF —Ñ–∞–π–ª—É
        lon: –î–æ–ª–≥–æ—Ç–∞ (EPSG:4326)
        lat: –®–∏—Ä–æ—Ç–∞ (EPSG:4326)
        
    Returns:
        Optional[float]: –ó–Ω–∞—á–µ–Ω–∏–µ NDVI –∏–ª–∏ None –µ—Å–ª–∏ –≤–Ω–µ —Ä–∞—Å—Ç—Ä–∞/nodata
    """
    _require_rasterio()
    
    try:
        with rasterio.open(tif_path) as src:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–æ—á–∫–∞ –≤ bounds
            if not (src.bounds.left <= lon <= src.bounds.right and
                    src.bounds.bottom <= lat <= src.bounds.top):
                return None
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º sample –¥–ª—è –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
            values = list(src.sample([(lon, lat)], indexes=1))
            
            if not values:
                return None
            
            val = float(values[0][0])
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
            if np.isnan(val) or np.isinf(val):
                return None
            
            # –ö–ª–∏–ø–ø–∏–Ω–≥ –∫ –≤–∞–ª–∏–¥–Ω–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É NDVI
            if val < -1.0 or val > 1.0:
                logger.warning(f"NDVI value {val} out of range [-1, 1], clipping")
                val = np.clip(val, -1.0, 1.0)
            
            return val
            
    except Exception as e:
        logger.warning(f"Failed to sample point ({lon}, {lat}): {e}")
        return None


def _as_float_or_none(x: Any) -> Optional[float]:
    """–ü—ã—Ç–∞–µ—Ç—Å—è –ø—Ä–∏–≤–µ—Å—Ç–∏ –∫ float, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –¥–ª—è None/NaN/–Ω–µ—á–∏—Å–µ–ª."""
    try:
        v = float(x)
        return v if np.isfinite(v) else None
    except (TypeError, ValueError):
        return None


def _stats_cache_key(
    bbox: List[float],
    start_date: str,
    end_date: str,
    aggregation_days: int
) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫—ç—à–∞ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ (SHA256 –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)."""
    payload = {
        "bbox": [round(b, 6) for b in bbox],
        "start": start_date,
        "end": end_date,
        "agg_days": aggregation_days
    }
    digest = hashlib.sha256(
        json.dumps(payload, sort_keys=True).encode("utf-8")
    ).hexdigest()[:16]  # First 16 chars for shorter filenames
    return f"stats_{digest}.json"




from math import cos, radians, ceil

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
S2L2A_MIN_MPP = settings.S2L2A_MIN_MPP    # —Ä–∞–∑—É–º–Ω—ã–π –º–∏–Ω–∏–º—É–º –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
S2L2A_MAX_MPP = settings.S2L2A_MAX_MPP    # –∂—ë—Å—Ç–∫–∏–π –ª–∏–º–∏—Ç Statistical API
MIN_PIXELS = settings.MIN_PIXELS          # —á—Ç–æ–±—ã –Ω–µ –ø–æ–ª—É—á–∏—Ç—å 1x1 px
MAX_PIXELS = settings.MAX_PIXELS          # –ø—Ä–µ–¥–æ—Ö—Ä–∞–Ω–∏—Ç–µ–ª—å –æ—Ç –≥–∏–≥–∞–Ω—Ç—Å–∫–∏—Ö –º–∞—Å—Å–∏–≤–æ–≤

def _approx_bbox_size_meters(bbox: List[float]) -> Tuple[float, float]:
    """–ì—Ä—É–±–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ bbox –≤ –º–µ—Ç—Ä–∞—Ö –ø–æ —à–∏—Ä–æ—Ç–µ —Å–µ—Ä–µ–¥–∏–Ω—ã –æ–∫–Ω–∞."""
    minx, miny, maxx, maxy = bbox
    lat_mid = (miny + maxy) / 2.0
    m_per_deg_lat = 111_320.0
    m_per_deg_lon = 111_320.0 * cos(radians(lat_mid))
    width_m  = max(1.0, (maxx - minx) * m_per_deg_lon)
    height_m = max(1.0, (maxy - miny) * m_per_deg_lat)
    return width_m, height_m

def _choose_resolution_and_size_for_s2(bbox: List[float], target_mpp: int = 60) -> Tuple[int, int]:
    """
    –í—ã–±–∏—Ä–∞–µ—Ç width/height —Ç–∞–∫, —á—Ç–æ–±—ã —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π meters-per-pixel –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ
    —É–∫–ª–∞–¥—ã–≤–∞–ª—Å—è –≤ –ª–∏–º–∏—Ç S2L2A_MAX_MPP. target_mpp ‚Äî –∂–µ–ª–∞–µ–º–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ.
    """
    w_m, h_m = _approx_bbox_size_meters(bbox)
    mpp = max(S2L2A_MIN_MPP, min(int(target_mpp), S2L2A_MAX_MPP))

    w_px = max(MIN_PIXELS, min(MAX_PIXELS, ceil(w_m / mpp)))
    h_px = max(MIN_PIXELS, min(MAX_PIXELS, ceil(h_m / mpp)))

    # –ö–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ‚Äî –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–æ–¥–Ω–∏–º–∞–µ–º —á–∏—Å–ª–æ –ø–∏–∫—Å–µ–ª–µ–π
    eff_mpp = max(w_m / w_px, h_m / h_px)
    if eff_mpp > S2L2A_MAX_MPP:
        w_px = max(w_px, min(MAX_PIXELS, ceil(w_m / S2L2A_MAX_MPP)))
        h_px = max(h_px, min(MAX_PIXELS, ceil(h_m / S2L2A_MAX_MPP)))

    logger.debug(f"S2 size select: bbox‚âà({w_m:.0f}x{h_m:.0f} m), target {mpp} m/px ‚Üí {w_px}x{h_px} px (eff‚âà{eff_mpp:.1f} m/px)")
    return int(w_px), int(h_px)





# --------------------- –î–æ–º–µ–Ω–Ω—ã–µ —Ö–µ–ª–ø–µ—Ä—ã/–∑–æ–Ω—ã ------------------------- #

def get_agricultural_zones(bbox: List[float]) -> List[Dict[str, Any]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–µ–ª—å—Å–∫–æ—Ö–æ–∑—è–π—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–æ–Ω –≤ bbox.
    
    Args:
        bbox: [minlon, minlat, maxlon, maxlat]
        
    Returns:
        List[Dict]: –°–ø–∏—Å–æ–∫ –∑–æ–Ω —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
    """
    zones = [
        {
            "name": "–°–µ–≤–µ—Ä–Ω–∞—è –∑–µ—Ä–Ω–æ–≤–∞—è –∑–æ–Ω–∞",
            "description": "–û—Å–Ω–æ–≤–Ω–∞—è –ø—à–µ–Ω–∏—á–Ω–∞—è –∑–æ–Ω–∞ —Ä–µ–≥–∏–æ–Ω–∞",
            "center": [52.28, 70.4],
            "area_ha": 1200000,
            "typical_crops": ["–ü—à–µ–Ω–∏—Ü–∞", "–Ø—á–º–µ–Ω—å", "–û–≤—ë—Å"]
        },
        {
            "name": "–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è —Å–º–µ—à–∞–Ω–Ω–∞—è –∑–æ–Ω–∞",
            "description": "–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω–æ–µ –∑–µ–º–ª–µ–¥–µ–ª–∏–µ",
            "center": [51.16, 71.45],
            "area_ha": 950000,
            "typical_crops": ["–ü—à–µ–Ω–∏—Ü–∞", "–ü–æ–¥—Å–æ–ª–Ω–µ—á–Ω–∏–∫", "–õ—ë–Ω"]
        },
        {
            "name": "–Æ–∂–Ω–∞—è –æ—Ä–æ—à–∞–µ–º–∞—è –∑–æ–Ω–∞",
            "description": "–ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–µ –æ—Ä–æ—à–∞–µ–º–æ–µ –∑–µ–º–ª–µ–¥–µ–ª–∏–µ",
            "center": [50.4, 72.3],
            "area_ha": 780000,
            "typical_crops": ["–ö—É–∫—É—Ä—É–∑–∞", "–û–≤–æ—â–∏", "–ë–∞—Ö—á–µ–≤—ã–µ"]
        }
    ]

    minlon, minlat, maxlon, maxlat = bbox
    filtered = []
    
    for zone in zones:
        lat, lon = zone["center"]
        if minlat <= lat <= maxlat and minlon <= lon <= maxlon:
            filtered.append(zone)

    return filtered if filtered else zones


def classify_ndvi_status(mean_ndvi: float) -> Dict[str, str]:
    """
    –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É NDVI.

    Args:
        mean_ndvi: –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ NDVI

    Returns:
        Dict: –°—Ç–∞—Ç—É—Å, —É—Ä–æ–≤–µ–Ω—å –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
    """
    if mean_ndvi < NDVI_THRESHOLD_CRITICAL:
        status = NDVI_STATUS_WATER
        level = "–í–æ–¥–∞"
        description = "–í–æ–¥–Ω–∞—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å"
    elif mean_ndvi < 0.2:
        status = NDVI_STATUS_BARE_SOIL
        level = "–û–≥–æ–ª—ë–Ω–Ω–∞—è –ø–æ—á–≤–∞"
        description = "–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏–ª–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"
    elif mean_ndvi < NDVI_THRESHOLD_HIGH:
        status = NDVI_STATUS_CRITICAL_LOW
        level = "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –Ω–∏–∑–∫–∏–π"
        description = "–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–∞—è —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≤–æ–∑–º–æ–∂–µ–Ω —Å—Ç—Ä–µ—Å—Å"
    elif mean_ndvi < 0.45:
        status = NDVI_STATUS_LOW
        level = "–ù–∏–∑–∫–∏–π"
        description = "–£–º–µ—Ä–µ–Ω–Ω–∞—è —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –Ω–∏–∂–µ –Ω–æ—Ä–º—ã"
    elif mean_ndvi < 0.65:
        status = NDVI_STATUS_OPTIMAL
        level = "–û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π"
        description = "–ó–¥–æ—Ä–æ–≤–∞—è —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"
    else:
        status = NDVI_STATUS_HIGH
        level = "–í—ã—Å–æ–∫–∏–π"
        description = "–û—á–µ–Ω—å –≥—É—Å—Ç–∞—è —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å"

    return {
        "status": status,
        "level": level,
        "description": description
    }


def _get_ndvi_statistics_evalscript() -> str:
    """
    Evalscript –¥–ª—è Statistical API —Å –º–æ–∑–∞–∏–∫–æ–π ORBIT –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.
    
    Returns:
        str: Evalscript V3 –∫–æ–¥
    """
    return """//VERSION=3
function setup() {
  return {
    input: [{
      bands: ["B04", "B08", "SCL", "dataMask"]
    }],
    output: [
      {
        id: "ndvi",
        bands: 1,
        sampleType: "FLOAT32"
      },
      {
        id: "dataMask",
        bands: 1
      }
    ],
    mosaicking: "ORBIT"
  };
}

function evaluatePixel(samples) {
  // –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—É—é –≤–∞–ª–∏–¥–Ω—É—é —Å—Ü–µ–Ω—É (–±–µ–∑ –æ–±–ª–∞–∫–æ–≤)
  for (let i = 0; i < samples.length; i++) {
    let sample = samples[i];
    
    // –ü—Ä–æ–≤–µ—Ä–∫–∞ dataMask
    if (sample.dataMask === 0) {
      continue;
    }
    
    // –ú–∞—Å–∫–∞ –æ–±–ª–∞–∫–æ–≤ –ø–æ SCL
    // 3=cloud shadows, 8=cloud medium probability, 9=cloud high probability, 10=thin cirrus, 11=snow
    if (sample.SCL === 3 || sample.SCL === 8 || sample.SCL === 9 || 
        sample.SCL === 10 || sample.SCL === 11) {
      continue;
    }
    
    // –í—ã—á–∏—Å–ª—è–µ–º NDVI
    let denom = sample.B08 + sample.B04;
    if (denom === 0) {
      continue;
    }
    
    let ndvi = (sample.B08 - sample.B04) / denom;
    ndvi = Math.max(-1, Math.min(1, ndvi));
    
    return {
      ndvi: [ndvi],
      dataMask: [1]
    };
  }
  
  // –í—Å–µ —Å—Ü–µ–Ω—ã –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω—ã
  return {
    ndvi: [NaN],
    dataMask: [0]
  };
}
"""


# -------------------------- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ NDVI -------------------------- #

def get_ndvi_statistics(
    bbox: List[float],
    start_date: str,
    end_date: str,
    aggregation_days: int = 5,
    use_cache: bool = True
) -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É NDVI –∑–∞ –ø–µ—Ä–∏–æ–¥ —á–µ—Ä–µ–∑ Statistical API.
    
    Args:
        bbox: [minlon, minlat, maxlon, maxlat] –≤ EPSG:4326
        start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)
        end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)
        aggregation_days: –ü–µ—Ä–∏–æ–¥ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –≤ –¥–Ω—è—Ö
        use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à
        
    Returns:
        Dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ NDVI —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ä—è–¥–æ–º
        
    Notes:
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Statistical API –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        –∑–∞ –≤–µ—Å—å –ø–µ—Ä–∏–æ–¥ –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º –≤–º–µ—Å—Ç–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ Process API –∑–∞–ø—Ä–æ—Å–æ–≤.
    """
    try:
        logger.info(
            f"NDVI statistics: bbox={bbox}, period={start_date}..{end_date}, "
            f"aggregation={aggregation_days}d"
        )
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if use_cache:
            cache_key = _stats_cache_key(bbox, start_date, end_date, aggregation_days)
            cache_path = STATS_CACHE_DIR / cache_key
            
            if cache_path.exists():
                try:
                    with open(cache_path, 'r') as f:
                        cached = json.load(f)
                    logger.info(f"Statistics cache hit: {cache_key}")
                    return cached
                except Exception as e:
                    logger.warning(f"Failed to load cache: {e}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω
        token = get_cdse_token()
        
        # Evalscript —Å ORBIT mosaicking
        evalscript = _get_ndvi_statistics_evalscript()

        # –ü–æ–¥–±–∏—Ä–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ç–∞–π–ª–∞ –ø–æ bbox, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç—å 1500 m/px
        width_px, height_px = _choose_resolution_and_size_for_s2(bbox, target_mpp=60)

        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∫ Statistical API
        payload = {
            "input": {
                "bounds": {
                    "bbox": bbox,
                    "properties": {
                        "crs": "http://www.opengis.net/def/crs/EPSG/0/4326"
                    }
                },
                "data": [{
                    "type": "sentinel-2-l2a",
                    "dataFilter": {
                        "maxCloudCoverage": 50  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π —Ñ–∏–ª—å—Ç—Ä, —Ç.–∫. –º–∞—Å–∫–∏—Ä—É–µ–º –≤ evalscript
                    },
                    "processing": {
                        "harmonizeValues": True
                    }
                }]
            },
            "aggregation": {
                "timeRange": {
                    "from": f"{start_date}T00:00:00Z",
                    "to": f"{end_date}T23:59:59Z"
                },
                "aggregationInterval": {
                    "of": f"P{aggregation_days}D"
                },
                "evalscript": evalscript,
                # –≤–º–µ—Å—Ç–æ resx/resy –∏—Å–ø–æ–ª—å–∑—É–µ–º —è–≤–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ä–∞—Å—Ç–µ—Ä–∞
                "width": width_px,
                "height": height_px
            },

            "calculations": {
                "default": {
                    "statistics": {
                        "default": {
                            "percentiles": {
                                "k": [10, 25, 50, 75, 90]
                            }
                        }
                    }
                }
            }
        }
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
        logger.info("Requesting statistics from Statistical API...")
        resp = requests.post(
            SH_STATISTICS_URL,
            headers=headers,
            json=payload,
            timeout=180
        )
        
        if resp.status_code == 400:
            error_text = resp.text
            if "no data" in error_text.lower():
                raise NoDataAvailableError(
                    f"No data available for {start_date} to {end_date}"
                )
            raise SentinelHubError(f"Invalid request: {error_text}")
        
        if resp.status_code != 200:
            logger.error(f"Statistical API error ({resp.status_code}): {resp.text}")
            resp.raise_for_status()
        
        result = resp.json()
        
        # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
        if result.get("status") != "OK":
            raise SentinelHubError(f"API returned non-OK status: {result}")
        
        data = result.get("data", [])
        
        if not data:
            raise NoDataAvailableError(
                f"No valid observations for {start_date} to {end_date}"
            )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º timeline
        timeline = []
        all_means = []
        
        for item in data:
            interval = item.get("interval", {})
            outputs = item.get("outputs", {})
            ndvi_output = outputs.get("ndvi", {})
            bands = ndvi_output.get("bands", {})
            band_stats = bands.get("B0", {}).get("stats", {})
            
            if not band_stats:
                continue
            
            mean_val = _as_float_or_none(band_stats.get("mean"))
            if mean_val is None:
                continue

            pcts = band_stats.get("percentiles", {}) or {}

            def _r(x):
                v = _as_float_or_none(x)
                return round(v, 3) if v is not None else None

            timeline.append({
                "date": (item.get("interval", {}).get("from", "")[:10]) or interval.get("from", "")[:10],
                "mean_ndvi": round(mean_val, 3),
                "min_ndvi": _r(band_stats.get("min")),
                "max_ndvi": _r(band_stats.get("max")),
                "std_ndvi": _r(band_stats.get("stDev")),
                "percentiles": {
                    "p10": _r(pcts.get("10.0")),
                    "p25": _r(pcts.get("25.0")),
                    "p50": _r(pcts.get("50.0")),
                    "p75": _r(pcts.get("75.0")),
                    "p90": _r(pcts.get("90.0")),
                }
            })
            all_means.append(float(mean_val))

        
        if not all_means:
            raise NoDataAvailableError(
                f"All observations masked (clouds/nodata) for {start_date} to {end_date}"
            )
        
        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        arr = np.asarray(all_means, dtype=np.float64)
        mean_ndvi   = float(np.nanmean(arr))
        median_ndvi = float(np.nanmedian(arr))
        std_ndvi    = float(np.nanstd(arr))
        min_ndvi    = float(np.nanmin(arr))
        max_ndvi    = float(np.nanmax(arr))

        
        # –¢—Ä–µ–Ω–¥
        if len(all_means) >= 3:
            x = np.arange(len(all_means))
            slope, _, r_value, p_value, _ = scipy_stats.linregress(x, all_means)
            
            if abs(slope) < 0.001:
                direction = "stable"
            else:
                direction = "increasing" if slope > 0 else "decreasing"
            
            trend = {
                "direction": direction,
                "slope": round(float(slope), 5),
                "r_squared": round(float(r_value ** 2), 3),
                "p_value": round(float(p_value), 4),
                "description": f"NDVI {direction} (R¬≤={round(r_value**2, 3)})"
            }
        else:
            trend = {
                "direction": "insufficient_data",
                "slope": 0.0,
                "r_squared": 0.0,
                "p_value": 1.0,
                "description": "Insufficient data for trend analysis"
            }
        
        status = classify_ndvi_status(mean_ndvi)
        
        response = {
            "status": STATUS_SUCCESS,
            "statistics": {
                "mean_ndvi": round(mean_ndvi, 3),
                "median_ndvi": round(median_ndvi, 3),
                "std_ndvi": round(std_ndvi, 3),
                "min_ndvi": round(min_ndvi, 3),
                "max_ndvi": round(max_ndvi, 3),
                "total_observations": len(all_means),
                "trend": trend,
                "status": status
            },
            "timeline": timeline,
            "products_available": len(data)
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        if use_cache:
            try:
                with open(cache_path, 'w') as f:
                    json.dump(response, f, indent=2)
                logger.info(f"Statistics cached: {cache_key}")
            except Exception as e:
                logger.warning(f"Failed to cache statistics: {e}")
        
        return response
        
    except NoDataAvailableError:
        raise
    except Exception as e:
        logger.error(f"NDVI statistics error: {e}", exc_info=True)
        return {
            "status": "error",
            "message": str(e),
            "statistics": {},
            "timeline": [],
            "products_available": 0
        }


# -------------------- –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ (–∫–ª–∞—Å—Å—ã NDVI) ---------------------- #

def get_ndvi_histogram(
    bbox: List[float],
    start_date: str,
    end_date: str,
    bins: Optional[List[float]] = None
) -> Dict[str, Any]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ NDVI –ø–æ –∫–ª–∞—Å—Å–∞–º —á–µ—Ä–µ–∑ Statistical API.
    
    Args:
        bbox: [minlon, minlat, maxlon, maxlat]
        start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)
        end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)
        bins: –ì—Ä–∞–Ω–∏—Ü—ã –±–∏–Ω–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: [-1, 0, 0.2, 0.3, 0.6, 1])
        
    Returns:
        Dict: –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
        
    Notes:
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Statistical API —Å histograms calculation –¥–ª—è
        —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ—Ä–æ–Ω–µ —Å–µ—Ä–≤–µ—Ä–∞.
    """
    try:
        if bins is None:
            bins = [-1.0, 0.0, 0.2, 0.3, 0.6, 1.0]
        
        logger.info(
            f"NDVI histogram: bbox={bbox}, period={start_date}..{end_date}"
        )
        
        token = get_cdse_token()
        evalscript = _get_ndvi_statistics_evalscript()

        # –î–ª—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –±–µ—Ä—ë–º —Ç–æ –∂–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
        width_px, height_px = _choose_resolution_and_size_for_s2(bbox, target_mpp=60)

        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å —Å histogram calculation
        payload = {
            "input": {
                "bounds": {
                    "bbox": bbox,
                    "properties": {
                        "crs": "http://www.opengis.net/def/crs/EPSG/0/4326"
                    }
                },
                "data": [{
                    "type": "sentinel-2-l2a",
                    "dataFilter": {
                        "maxCloudCoverage": 50
                    },
                    "processing": {
                        "harmonizeValues": True
                    }
                }]
            },
            "aggregation": {
                "timeRange": {
                    "from": f"{start_date}T00:00:00Z",
                    "to": f"{end_date}T23:59:59Z"
                },
                "aggregationInterval": {
                    "of": f"P{(datetime.strptime(end_date, '%Y-%m-%d') - datetime.strptime(start_date, '%Y-%m-%d')).days}D"
                },
                "evalscript": evalscript,
                "width": width_px,
                "height": height_px
            },

            "calculations": {
                "ndvi": {
                    "histograms": {
                        "default": {
                            "bins": bins
                        }
                    }
                }
            }
        }
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        resp = requests.post(
            SH_STATISTICS_URL,
            headers=headers,
            json=payload,
            timeout=180
        )
        
        if resp.status_code != 200:
            logger.error(f"Histogram API error ({resp.status_code}): {resp.text}")
            if resp.status_code == 400 and "no data" in resp.text.lower():
                raise NoDataAvailableError(f"No data for {start_date} to {end_date}")
            resp.raise_for_status()
        
        result = resp.json()
        
        if result.get("status") != "OK":
            raise SentinelHubError(f"API returned non-OK status: {result}")
        
        data = result.get("data", [])
        
        if not data:
            raise NoDataAvailableError("No valid data for histogram")
        
        # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π) –∏–Ω—Ç–µ—Ä–≤–∞–ª
        outputs = data[0].get("outputs", {})
        ndvi_output = outputs.get("ndvi", {})
        bands = ndvi_output.get("bands", {})
        histogram = bands.get("B0", {}).get("histogram", {})
        
        hist_bins = histogram.get("bins", [])
        
        if not hist_bins:
            raise SentinelHubError("Empty histogram returned")
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        total = sum(b.get("count", 0) for b in hist_bins)
        
        formatted_bins = []
        for i, bin_data in enumerate(hist_bins):
            count = bin_data.get("count", 0)
            low = float(bin_data.get("lowEdge", bins[i]))
            high = float(bin_data.get("highEdge", bins[i + 1]))
            pct = (count / total * 100.0) if total > 0 else 0.0
            
            # –ß–∏—Ç–∞–µ–º—ã–µ –ø–æ–¥–ø–∏—Å–∏
            if i == 0 and low <= -1:
                label = "< 0"
            elif i == len(hist_bins) - 1 and high >= 1:
                label = f"{low:.1f}+"
            else:
                label = f"{low:.1f}‚Äì{high:.1f}"
            
            formatted_bins.append({
                "min": low,
                "max": high,
                "count": int(count),
                "pct": round(pct, 2),
                "label": label
            })
        
        return {
            "status": "success",
            "bins": formatted_bins,
            "total": total,
            "overflow": histogram.get("overflowCount", 0),
            "underflow": histogram.get("underflowCount", 0)
        }
        
    except NoDataAvailableError:
        raise
    except Exception as e:
        logger.error(f"NDVI histogram error: {e}", exc_info=True)
        return {
            "status": "error",
            "message": str(e),
            "bins": []
        }


# -------------------- –¢–∞–π–º-—Å–µ—Ä–∏—è –ø–æ —Ç–æ—á–∫–µ ----------------------------- #

def get_point_timeseries(
    lon: float,
    lat: float,
    bbox: List[float],
    start_date: str,
    end_date: str,
    max_dates: int = 20
) -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Ç–∞–π–º-—Å–µ—Ä–∏—é NDVI –≤ —Ç–æ—á–∫–µ (lon, lat).
    
    Args:
        lon: –î–æ–ª–≥–æ—Ç–∞ (EPSG:4326)
        lat: –®–∏—Ä–æ—Ç–∞ (EPSG:4326)
        bbox: [minlon, minlat, maxlon, maxlat] –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)
        end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)
        max_dates: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞—Ç
        
    Returns:
        Dict: –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ NDVI –≤ —Ç–æ—á–∫–µ
        
    Notes:
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç Statistical API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞,
        –∑–∞—Ç–µ–º –±–µ—Ä—ë—Ç –æ–¥–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ –¥–∞—Ç—É –∏ —Å—ç–º–ø–ª–∏—Ä—É–µ—Ç —Ç–æ—á–∫—É.
        –î–ª—è –±–æ–ª—å—à–µ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∞–ª–µ–Ω—å–∫–∏–π bbox –≤–æ–∫—Ä—É–≥ —Ç–æ—á–∫–∏.
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞: —Ç–æ—á–∫–∞ –≤–Ω—É—Ç—Ä–∏ bbox
        minlon, minlat, maxlon, maxlat = bbox
        if not (minlon <= lon <= maxlon and minlat <= lat <= maxlat):
            return {
                "status": "error",
                "message": "Point outside bbox",
                "series": []
            }
        
        logger.info(
            f"Point timeseries: ({lon}, {lat}), period={start_date}..{end_date}"
        )
        
        # –°–æ–∑–¥–∞—ë–º –º–∞–ª–µ–Ω—å–∫–∏–π bbox –≤–æ–∫—Ä—É–≥ —Ç–æ—á–∫–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        # ~1km –Ω–∞ —ç–∫–≤–∞—Ç–æ—Ä–µ ‚âà 0.01 –≥—Ä–∞–¥—É—Å–∞
        buffer = 0.01
        point_bbox = [
            lon - buffer,
            lat - buffer,
            lon + buffer,
            lat + buffer
        ]

        # –î–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –æ–∫–Ω–∞ –≤–æ–∫—Ä—É–≥ —Ç–æ—á–∫–∏ –º–æ–∂–Ω–æ –ø–æ–∑–≤–æ–ª–∏—Ç—å 10 –º/px
        width_px, height_px = _choose_resolution_and_size_for_s2(point_bbox, target_mpp=10)

        
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–∞—Ç —Å–æ Statistical API
        token = get_cdse_token()
        evalscript = _get_ndvi_statistics_evalscript()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º aggregation interval
        date_range = (
            datetime.strptime(end_date, "%Y-%m-%d") -
            datetime.strptime(start_date, "%Y-%m-%d")
        ).days
        
        # –ü–æ–¥–±–∏—Ä–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–Ω–æ max_dates —Ç–æ—á–µ–∫
        if date_range <= max_dates:
            agg_days = 1
        else:
            agg_days = max(1, date_range // max_dates)
        
        payload = {
            "input": {
                "bounds": {
                    "bbox": point_bbox,
                    "properties": {
                        "crs": "http://www.opengis.net/def/crs/EPSG/0/4326"
                    }
                },
                "data": [{
                    "type": "sentinel-2-l2a",
                    "dataFilter": {
                        "maxCloudCoverage": 50
                    },
                    "processing": {
                        "harmonizeValues": True
                    }
                }]
            },
            "aggregation": {
                "timeRange": {
                    "from": f"{start_date}T00:00:00Z",
                    "to": f"{end_date}T23:59:59Z"
                },
                "aggregationInterval": {
                    "of": f"P{agg_days}D"
                },
                "evalscript": evalscript,
                "width": width_px,
                "height": height_px
            }

        }
        
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json",
            "Accept": "application/json"
        }
        
        resp = requests.post(
            SH_STATISTICS_URL,
            headers=headers,
            json=payload,
            timeout=120
        )
        
        if resp.status_code != 200:
            logger.error(f"Point timeseries API error: {resp.text}")
            if "no data" in resp.text.lower():
                return {
                    "status": "success",
                    "series": [],
                    "message": "No data available for this point"
                }
            resp.raise_for_status()
        
        result = resp.json()
        data = result.get("data", [])
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–µ—Ä–∏—é
        series = []
        for item in data:
            interval = item.get("interval", {})
            date_str = interval.get("from", "")[:10]
            
            outputs = item.get("outputs", {})
            ndvi_output = outputs.get("ndvi", {})
            bands = ndvi_output.get("bands", {})
            stats = bands.get("B0", {}).get("stats", {})

            mean_val = _as_float_or_none(stats.get("mean"))
            if mean_val is not None:
                series.append({
                    "date": date_str,
                    "ndvi": round(mean_val, 3)
                })
        
        return {
            "status": "success",
            "series": series,
            "location": {"lon": lon, "lat": lat}
        }
        
    except Exception as e:
        logger.error(f"Point timeseries error: {e}", exc_info=True)
        return {
            "status": "error",
            "message": str(e),
            "series": []
        }


# --------------------------- –û—Ç—á—ë—Ç NDVI ------------------------------ #

def generate_recommendations(
    mean_ndvi: float,
    statistics: Dict[str, Any],
    timeline: List[Dict[str, Any]]
) -> List[str]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ NDVI —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.
    
    Args:
        mean_ndvi: –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ NDVI
        statistics: –°–ª–æ–≤–∞—Ä—å —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
        timeline: –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥
        
    Returns:
        List[str]: –°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    """
    recommendations = []
    
    trend = statistics.get("trend", {})
    trend_direction = trend.get("direction", "stable")
    r_squared = trend.get("r_squared", 0)
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É—Ä–æ–≤–Ω—é NDVI
    if mean_ndvi < 0.3:
        recommendations.append(
            "‚ö†Ô∏è –ù–∏–∑–∫–∏–π NDVI: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ—Å–µ–≤—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å—Ç—Ä–µ—Å—Å–∞ "
            "(–∑–∞—Å—É—Ö–∞, –≤—Ä–µ–¥–∏—Ç–µ–ª–∏, –±–æ–ª–µ–∑–Ω–∏)"
        )
        recommendations.append(
            "üíß –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –æ—Ä–æ—à–µ–Ω–∏—è –∏–ª–∏ "
            "–≤–Ω–µ—Å–µ–Ω–∏—è —É–¥–æ–±—Ä–µ–Ω–∏–π"
        )
        recommendations.append(
            "üìä –ü—Ä–æ–≤–µ–¥–∏—Ç–µ –ø–æ—á–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –¥–µ—Ñ–∏—Ü–∏—Ç–∞ –ø–∏—Ç–∞—Ç–µ–ª—å–Ω—ã—Ö –≤–µ—â–µ—Å—Ç–≤"
        )
    elif mean_ndvi < 0.45:
        recommendations.append(
            "‚ö° NDVI –Ω–∏–∂–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ: –º–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Å–µ–≤–æ–≤ "
            "–∫–∞–∂–¥—ã–µ 5‚Äì7 –¥–Ω–µ–π"
        )
        recommendations.append(
            "üå°Ô∏è –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –æ—Å–∞–¥–∫–∞–º –∏ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ –∑–∞ –ø–µ—Ä–∏–æ–¥"
        )
    else:
        recommendations.append(
            "‚úÖ NDVI –≤ –Ω–æ—Ä–º–µ: –ø—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞–∂–¥—ã–µ 10‚Äì14 –¥–Ω–µ–π"
        )
    
    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ç—Ä–µ–Ω–¥—É
    if trend_direction == "decreasing" and r_squared > 0.5:
        recommendations.append(
            "üìâ –¢—Ä–µ–Ω–¥ —Å–Ω–∏–∂–µ–Ω–∏—è NDVI: —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω —É—Ö—É–¥—à–µ–Ω–∏—è"
        )
        recommendations.append(
            "üîç –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏—Å—Ç–æ—Ä–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–ª–µ–π –∏ –ø–æ–≥–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è"
        )
    elif trend_direction == "increasing" and r_squared > 0.5:
        recommendations.append(
            "üìà –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥: —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä–∞—Å—Ç–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —É–ª—É—á—à–∞–µ—Ç—Å—è"
        )
    elif trend_direction == "stable":
        recommendations.append(
            "‚û°Ô∏è –°—Ç–∞–±–∏–ª—å–Ω—ã–π NDVI: –º–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –¥–∞–ª—å–Ω–µ–π—à—É—é –¥–∏–Ω–∞–º–∏–∫—É"
        )
    
    # –í–∞—Ä–∏–∞–±–µ–ª—å–Ω–æ—Å—Ç—å
    std_ndvi = statistics.get("std_ndvi", 0)
    if std_ndvi > 0.15:
        recommendations.append(
            "üìä –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞–±–µ–ª—å–Ω–æ—Å—Ç—å NDVI: –≤–æ–∑–º–æ–∂–Ω–∞ –Ω–µ–æ–¥–Ω–æ—Ä–æ–¥–Ω–æ—Å—Ç—å –ø–æ–ª–µ–π "
            "–∏–ª–∏ –∏–∑–º–µ–Ω—á–∏–≤—ã–µ —É—Å–ª–æ–≤–∏—è"
        )
    
    # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    recommendations.append(
        "üìÖ –°—Ä–∞–≤–Ω–∏—Ç–µ —Ç–µ–∫—É—â–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ —Å –¥–∞–Ω–Ω—ã–º–∏ –ø—Ä–æ—à–ª—ã—Ö –ª–µ—Ç –¥–ª—è "
        "–≤—ã—è–≤–ª–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π"
    )
    recommendations.append(
        "üõ∞Ô∏è –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º—É–ª—å—Ç–∏—Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"
    )
    
    return recommendations


def generate_ndvi_report(
    bbox: List[float],
    date: str,
    period_days: int = 30
) -> Dict[str, Any]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á—ë—Ç –ø–æ NDVI –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥ –¥–æ –¥–∞—Ç—ã.
    
    Args:
        bbox: [minlon, minlat, maxlon, maxlat]
        date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ (YYYY-MM-DD)
        period_days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –Ω–∞–∑–∞–¥ –æ—Ç –¥–∞—Ç—ã
        
    Returns:
        Dict: –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
    """
    try:
        end_date = datetime.strptime(date, "%Y-%m-%d")
        start_date = end_date - timedelta(days=period_days)
        
        start_str = start_date.strftime("%Y-%m-%d")
        end_str = end_date.strftime("%Y-%m-%d")
        
        logger.info(f"Generating NDVI report for {start_str} to {end_str}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats_data = get_ndvi_statistics(
            bbox=bbox,
            start_date=start_str,
            end_date=end_str,
            aggregation_days=5
        )
        
        if stats_data["status"] != "success":
            return {
                "status": "error",
                "message": "Failed to generate report",
                "region": "–ê–∫–º–æ–ª–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
                "report_date": date
            }
        
        statistics = stats_data["statistics"]
        timeline = stats_data["timeline"]
        mean_ndvi = statistics.get("mean_ndvi", 0.0)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
        status_info = classify_ndvi_status(mean_ndvi)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        recommendations = generate_recommendations(
            mean_ndvi, statistics, timeline
        )
        
        # –°–µ–ª—å—Å–∫–æ—Ö–æ–∑—è–π—Å—Ç–≤–µ–Ω–Ω—ã–µ –∑–æ–Ω—ã
        zones = get_agricultural_zones(bbox)
        
        return {
            "status": "success",
            "region": "–ê–∫–º–æ–ª–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
            "report_date": date,
            "period_analyzed": f"{start_str} ‚Äì {end_str}",
            "vegetation_status": {
                "overall": status_info["level"],
                "description": status_info["description"],
                "trend": statistics.get("trend", {}).get("description", ""),
                "recommendations": recommendations
            },
            "ndvi_statistics": {
                "mean_ndvi": statistics.get("mean_ndvi", 0.0),
                "median_ndvi": statistics.get("median_ndvi", 0.0),
                "std_ndvi": statistics.get("std_ndvi", 0.0),
                "min_ndvi": statistics.get("min_ndvi", 0.0),
                "max_ndvi": statistics.get("max_ndvi", 0.0),
                "observations_count": statistics.get("total_observations", 0)
            },
            "timeline": timeline,
            "agricultural_zones": zones,
            "products_available": stats_data.get("products_available", 0)
        }
        
    except NoDataAvailableError as e:
        logger.error(f"No data for report: {e}")
        return {
            "status": "error",
            "message": str(e),
            "region": "–ê–∫–º–æ–ª–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
            "report_date": date
        }
    except Exception as e:
        logger.error(f"Report generation error: {e}", exc_info=True)
        return {
            "status": "error",
            "message": str(e),
            "region": "–ê–∫–º–æ–ª–∏–Ω—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å",
            "report_date": date
        }


# ----------------------- –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è batch –æ–ø–µ—Ä–∞—Ü–∏–π ------------------- #

def get_multiple_points_timeseries(
    points: List[Tuple[float, float]],
    bbox: List[float],
    start_date: str,
    end_date: str,
    max_dates: int = 20
) -> Dict[str, Any]:
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Ç–∞–π–º-—Å–µ—Ä–∏–∏ NDVI –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ—á–µ–∫.
    
    Args:
        points: –°–ø–∏—Å–æ–∫ (lon, lat) –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        bbox: –û–±—â–∏–π bbox –¥–ª—è –≤—Å–µ—Ö —Ç–æ—á–µ–∫
        start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞
        end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞
        max_dates: –ú–∞–∫—Å–∏–º—É–º –¥–∞—Ç –Ω–∞ —Ç–æ—á–∫—É
        
    Returns:
        Dict: –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –¥–ª—è –≤—Å–µ—Ö —Ç–æ—á–µ–∫
    """
    results = []
    
    for i, (lon, lat) in enumerate(points):
        try:
            series = get_point_timeseries(
                lon=lon,
                lat=lat,
                bbox=bbox,
                start_date=start_date,
                end_date=end_date,
                max_dates=max_dates
            )
            results.append({
                "point_id": i,
                "lon": lon,
                "lat": lat,
                "series": series.get("series", []),
                "status": series.get("status", "error")
            })
        except Exception as e:
            logger.error(f"Failed to get series for point {i} ({lon}, {lat}): {e}")
            results.append({
                "point_id": i,
                "lon": lon,
                "lat": lat,
                "series": [],
                "status": "error",
                "error": str(e)
            })
    
    return {
        "status": "success",
        "points": results,
        "total_points": len(points)
    }